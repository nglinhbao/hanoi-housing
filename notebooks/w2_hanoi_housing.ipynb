{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dự Đoán Giá Nhà từ Dữ Liệu Bất Động Sản ở Hà Nội (Tuần 2)**"
      ],
      "metadata": {
        "id": "4XlUhgWUpgK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ở bài trước, chúng ta đã chạy được lan truyền xuôi dựa trên tập dữ liệu bất động sản ở Hà Nội. Trong bài tập này, chúng ta sẽ đi sâu hơn về lan truyền ngược. Trong phạm vi của lan truyền ngược, chúng ta chỉ viết các hàm (functions), và chạy qua 1 lượt lan truyền ngược mẫu, phần huấn luyện mô hình còn lại sẽ nằm ở tuần 3.\n",
        "\n",
        "**Dữ Liệu**:\n",
        "\n",
        "Chúng ta sẽ sử dụng tập dữ liệu về dự đoán giá nhà được lấy từ Kaggle (https://www.kaggle.com/datasets/ladcva/vietnam-housing-dataset-hanoi/data). Tập dữ liệu này chứa thông tin về các giao dịch bất động sản ở Hà Nội, bao gồm các thuộc tính liên quan như địa chỉ, quận, huyện, diện tích, số phòng ngủ,... và giá bán. Mục tiêu của chúng ta là xây dựng một mô hình để dự đoán giá của các căn nhà dựa trên các thuộc tính này.\n",
        "\n",
        "**Phương Pháp**:\n",
        "\n",
        "Chúng ta sẽ tiến hành các bước sau:\n",
        "* Hàm mất mát MSE\n",
        "* Lan truyền ngược"
      ],
      "metadata": {
        "id": "WbKY5K3gpdU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "mEIQbA5lksmm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BÀI TẬP: Tính độ mất mát\n",
        "Trong cell này, chúng ta định nghĩa hàm compute_cost để tính toán độ mất mát của mô hình bằng phương pháp Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "OqVV_UhDEf_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Giải thích Mean Squared Error (MSE):\n",
        "\n",
        "Mean Squared Error là trung bình của tổng các bình phương của sự khác biệt giữa giá trị dự đoán $A$ và giá trị thực tế $Y$.\n",
        "Công thức:\n",
        "\\begin{equation*}\n",
        "\\text{MSE} = \\frac{1}{2m} \\sum_{i=1}^{m} (A_i - Y_i)^2\n",
        "\\end{equation*}\n",
        "\n",
        "Trong đó:\n",
        "- $m$ là số lượng ví dụ.\n",
        "- $A_i$ là giá trị dự đoán cho ví dụ thứ $i$.\n",
        "- $Y_i$ là giá trị thực cho ví dụ thứ $i$.\n",
        "2\n",
        "\n",
        "Gợi ý: sử dụng hàm [numpy.sum](https://numpy.org/doc/2.0/reference/generated/numpy.sum.html) và [numpy.square](https://numpy.org/doc/stable/reference/generated/numpy.square.html)"
      ],
      "metadata": {
        "id": "OGx5IujUEmNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tính toán độ mất mát (mean squared error)\n",
        "def compute_cost(A, Y):\n",
        "    \"\"\"\n",
        "    Tính toán lỗi bình phương trung bình.\n",
        "\n",
        "    Tham số:\n",
        "    A -- Đầu ra của kích hoạt cuối cùng, có dạng (1, số lượng ví dụ)\n",
        "    Y -- vector nhãn \"thật\" có dạng (1, số lượng ví dụ)\n",
        "\n",
        "    Trả về:\n",
        "    cost -- lỗi bình phương trung bình\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    ### BEGIN SOLUTION\n",
        "    cost = (1.0 / (2 * m)) * np.sum(np.square(A - Y))\n",
        "    ### END SOLUTION\n",
        "    return float(np.squeeze(cost))"
      ],
      "metadata": {
        "id": "HOoqX2hiBAsr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests 10 points.\n",
        "# Test Case 1: Perfect predictions\n",
        "def test_case_1():\n",
        "    Y = np.array([[1, 0, 1, 1, 0]])\n",
        "    A = np.array([[1, 0, 1, 1, 0]])\n",
        "    cost = compute_cost(A, Y)\n",
        "    assert np.isclose(cost, 0), f\"Cost should be 0 for perfect predictions, but got {cost}\"\n",
        "    print(\"Test Case 1 passed!\")\n",
        "\n",
        "test_case_1()\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "# Test Case 2: Completely wrong predictions\n",
        "def test_case_2():\n",
        "    Y = np.array([[1, 0, 1, 1, 0]])\n",
        "    A = np.array([[0, 1, 0, 0, 1]])\n",
        "    cost = compute_cost(A, Y)\n",
        "    assert np.isclose(cost, 0.5), f\"Cost should be 0.5 for completely wrong predictions, but got {cost}\"\n",
        "    print(\"Test Case 2 passed!\")\n",
        "\n",
        "# Test Case 3: Mixed predictions\n",
        "def test_case_3():\n",
        "    Y = np.array([[1, 0, 1, 1, 0]])\n",
        "    A = np.array([[0.8, 0.2, 0.7, 0.9, 0.1]])\n",
        "    expected_cost = 0.019000000000000003\n",
        "    cost = compute_cost(A, Y)\n",
        "    assert np.isclose(cost, expected_cost, atol=1e-4), f\"Expected cost {expected_cost}, but got {cost}\"\n",
        "    print(\"Test Case 3 passed!\")\n",
        "\n",
        "# Test Case 4: Large input\n",
        "def test_case_4():\n",
        "    np.random.seed(42)\n",
        "    Y = np.random.randint(0, 2, (1, 1000))\n",
        "    A = np.random.rand(1, 1000)\n",
        "    cost = compute_cost(A, Y)\n",
        "    assert 0 <= cost <= 0.5, f\"Cost should be between 0 and 0.5, but got {cost}\"\n",
        "    print(\"Test Case 4 passed!\")\n",
        "\n",
        "# Test Case 5: Edge case - all zeros\n",
        "def test_case_5():\n",
        "    Y = np.zeros((1, 100))\n",
        "    A = np.zeros((1, 100))\n",
        "    cost = compute_cost(A, Y)\n",
        "    assert np.isclose(cost, 0), f\"Cost should be 0 for all zeros, but got {cost}\"\n",
        "    print(\"Test Case 5 passed!\")\n",
        "\n",
        "# Run all test cases\n",
        "test_case_2()\n",
        "test_case_3()\n",
        "test_case_4()\n",
        "test_case_5()\n",
        "### END HIDDEN TESTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRwR9fmT3r2t",
        "outputId": "5d84cbc2-b1e0-45cc-ed68-a0e0ad7e6bff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1 passed!\n",
            "Test Case 2 passed!\n",
            "Test Case 3 passed!\n",
            "Test Case 4 passed!\n",
            "Test Case 5 passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BÀI TẬP: Lan truyền ngược\n",
        "Trong cell này, chúng ta định nghĩa hàm backward_propagation để thực hiện lan truyền ngược cho một lớp của mạng neural. Lan truyền ngược giúp tính toán gradient của hàm mất mát đối với các tham số của mạng, qua đó cập nhật trọng số và bias để tối ưu hóa mô hình."
      ],
      "metadata": {
        "id": "PxEP5S50Fcas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là cách bạn có thể viết lại các công thức theo định dạng mà bạn yêu cầu:\n",
        "\n",
        "### Giải thích Chi Tiết Lan Truyền Ngược Dựa Vào Chain Rule\n",
        "\n",
        "#### Backward Propagation\n",
        "\n",
        "Quá trình lan truyền ngược dựa vào quy tắc chuỗi để tính toán gradient của hàm mất mát $L$ với từng tham số $W$, $b$, và $A$.\n",
        "\n",
        "##### Đạo Hàm Với Lớp Đầu Ra (Output Layer)\n",
        "\n",
        "- **Tính $dZ^{[2]}$**\n",
        "\\begin{equation*}\n",
        "   dZ^{[2]} = A^{[2]} - Y\n",
        "\\end{equation*}\n",
        " - Đây là gradient của hàm mất mát $L$ đối với $Z^{[2]}$. Nếu hàm mất mát là hàm bình phương (mean squared error), thì $dZ^{[2]} = A^{[2]} - Y$.\n",
        "\n",
        "- **Tính $dW^{[2]}$**\n",
        "\\begin{equation*}\n",
        "   dW^{[2]} = \\frac{1}{m} dZ^{[2]} (A^{[1]})^T\n",
        "\\end{equation*}\n",
        " - Sử dụng quy tắc chuỗi, chúng ta có $dW^{[2]} = \\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial Z^{[2]}} \\cdot \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}$.\n",
        " - $\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} = A^{[1]}$, do đó $dW^{[2]} = \\frac{1}{m} dZ^{[2]} (A^{[1]})^T$.\n",
        "\n",
        "- **Tính $db^{[2]}$**\n",
        "\\begin{equation*}\n",
        "   db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}\n",
        "\\end{equation*}\n",
        " - Gradient của hàm mất mát $L$ đối với bias $b^{[2]}$ là tổng của tất cả các gradient của $Z^{[2]}$ chia cho số lượng ví dụ $m$.\n",
        "\n",
        "##### Đạo Hàm Với Lớp Ẩn (Hidden Layer)\n",
        "\n",
        "- **Tính $dZ^{[1]}$**\n",
        "\\begin{equation*}\n",
        "   dZ^{[1]} = W^{[2]T} dZ^{[2]} \\odot g'^{[1]}(Z^{[1]})\n",
        "\\end{equation*}\n",
        " - Sử dụng quy tắc chuỗi, gradient của hàm mất mát $L$ đối với $Z^{[1]}$ là:\n",
        "\\begin{equation*}\n",
        "dZ^{[1]} = \\frac{\\partial L}{\\partial A^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}\n",
        "\\end{equation*}\n",
        " - $\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} = g'^{[1]}(Z^{[1]})$.\n",
        " - $\\frac{\\partial L}{\\partial A^{[1]}} = W^{[2]T} dZ^{[2]}$.\n",
        "\n",
        "- **Tính $dW^{[1]}$**\n",
        "\\begin{equation*}\n",
        "   dW^{[1]} = \\frac{1}{m} dZ^{[1]} (X^T)\n",
        "\\end{equation*}\n",
        " - Sử dụng quy tắc chuỗi, $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial Z^{[1]}} \\cdot \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}$.\n",
        " - $\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} = X$.\n",
        "\n",
        "- **Tính $db^{[1]}$**\n",
        "\\begin{equation*}\n",
        "   db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}\n",
        "\\end{equation*}\n",
        " - Tương tự như với lớp đầu ra, gradient của hàm mất mát $L$ đối với bias $b^{[1]}$ là tổng của tất cả các gradient của $Z^{[1]}$ chia cho số lượng ví dụ $m$.\n",
        "\n",
        "### Tóm Tắt Bằng Công Thức\n",
        "\n",
        "- **Tính $dZ^{[2]}$**\n",
        "\\begin{equation*}\n",
        "   dZ^{[2]} = A^{[2]} - Y\n",
        "\\end{equation*}\n",
        "\n",
        "- **Tính $dW^{[2]}$**\n",
        "\\begin{equation*}\n",
        "   dW^{[2]} = \\frac{1}{m} dZ^{[2]} (A^{[1]})^T\n",
        "\\end{equation*}\n",
        "\n",
        "- **Tính $db^{[2]}$**\n",
        "\\begin{equation*}\n",
        "   db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}\n",
        "\\end{equation*}\n",
        "\n",
        "- **Tính $dZ^{[1]}$**\n",
        "\\begin{equation*}\n",
        "   dZ^{[1]} = W^{[2]T} dZ^{[2]} \\odot g'^{[1]}(Z^{[1]})\n",
        "\\end{equation*}\n",
        "\n",
        "- **Tính $dW^{[1]}$**\n",
        "\\begin{equation*}\n",
        "   dW^{[1]} = \\frac{1}{m} dZ^{[1]} (X^T)\n",
        "\\end{equation*}\n",
        "\n",
        "- **Tính $db^{[1]}$**\n",
        "\\begin{equation*}\n",
        "   db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "3z7IiYxdFgup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Đạo hàm của ReLU"
      ],
      "metadata": {
        "id": "6VcyKAGB5onW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm `relu_derivative(x)` được dùng để tính đạo hàm của hàm kích hoạt ReLU. Đối với các giá trị dương của đầu vào `x`, đạo hàm sẽ bằng 1, và với các giá trị không dương, đạo hàm sẽ bằng 0. Để đảm bảo tính linh hoạt khi xử lý cả giá trị đơn lẻ và mảng, hàm sử dụng [np.where](https://numpy.org/doc/2.1/reference/generated/numpy.where.html) để thực hiện phép kiểm tra điều kiện."
      ],
      "metadata": {
        "id": "lpgEwdNDjtnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(x):\n",
        "    ### BEGIN SOLUTION\n",
        "    # Use np.where to handle both scalars and arrays\n",
        "    return np.where(x > 0, 1, 0)\n",
        "    ### END SOLUTION"
      ],
      "metadata": {
        "id": "LoI0A5mn5q7x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests 10 points.\n",
        "\n",
        "def relu_derivative(x):\n",
        "    # Use np.where to handle both scalars and arrays\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Test Case 1: Basic scalar inputs\n",
        "def test_case_1():\n",
        "    assert relu_derivative(5.0) == 1, \"Failed on positive scalar input\"\n",
        "    assert relu_derivative(-5.0) == 0, \"Failed on negative scalar input\"\n",
        "    assert relu_derivative(0) == 0, \"Failed on zero input\"\n",
        "    print(\"Test Case 1 passed!\")\n",
        "\n",
        "test_case_1()\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "\n",
        "# Test Case 2: 1D array input\n",
        "def test_case_2():\n",
        "    x = np.array([-2, -1, 0, 1, 2])\n",
        "    expected = np.array([0, 0, 0, 1, 1])\n",
        "    result = relu_derivative(x)\n",
        "    assert np.array_equal(result, expected), \"Failed on 1D array input\"\n",
        "    print(\"Test Case 2 passed!\")\n",
        "\n",
        "# Test Case 3: 2D array input\n",
        "def test_case_3():\n",
        "    x = np.array([[-1, 0, 1], [2, -3, 4]])\n",
        "    expected = np.array([[0, 0, 1], [1, 0, 1]])\n",
        "    result = relu_derivative(x)\n",
        "    assert np.array_equal(result, expected), \"Failed on 2D array input\"\n",
        "    print(\"Test Case 3 passed!\")\n",
        "\n",
        "# Test Case 4: Float array input\n",
        "def test_case_4():\n",
        "    x = np.array([-0.5, 0.0, 0.5])\n",
        "    expected = np.array([0, 0, 1])\n",
        "    result = relu_derivative(x)\n",
        "    assert np.array_equal(result, expected), \"Failed on float array input\"\n",
        "    print(\"Test Case 4 passed!\")\n",
        "\n",
        "# Test Case 5: Large values\n",
        "def test_case_5():\n",
        "    x = np.array([-1e6, 1e6])\n",
        "    expected = np.array([0, 1])\n",
        "    result = relu_derivative(x)\n",
        "    assert np.array_equal(result, expected), \"Failed on large values\"\n",
        "    print(\"Test Case 5 passed!\")\n",
        "\n",
        "# Test Case 6: Small values\n",
        "def test_case_6():\n",
        "    x = np.array([-1e-6, 1e-6])\n",
        "    expected = np.array([0, 1])\n",
        "    result = relu_derivative(x)\n",
        "    assert np.array_equal(result, expected), \"Failed on small values\"\n",
        "    print(\"Test Case 6 passed!\")\n",
        "\n",
        "# Test Case 7: Empty array\n",
        "def test_case_7():\n",
        "    x = np.array([])\n",
        "    result = relu_derivative(x)\n",
        "    assert len(result) == 0, \"Failed on empty array length check\"\n",
        "    assert isinstance(result, np.ndarray), \"Failed on empty array type check\"\n",
        "    print(\"Test Case 7 passed!\")\n",
        "\n",
        "# Test Case 8: Broadcasting behavior\n",
        "def test_case_8():\n",
        "    x = np.array([[1, -1], [-1, 1]])\n",
        "    expected = np.array([[1, 0], [0, 1]])\n",
        "    result = relu_derivative(x)\n",
        "    assert np.array_equal(result, expected), \"Failed on broadcasting test\"\n",
        "    print(\"Test Case 8 passed!\")\n",
        "\n",
        "test_case_2()\n",
        "test_case_3()\n",
        "test_case_4()\n",
        "test_case_5()\n",
        "test_case_6()\n",
        "test_case_7()\n",
        "test_case_8()\n",
        "\n",
        "### END HIDDEN TESTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_cSXxOhkUEQ",
        "outputId": "20f8ee2e-fc56-4d57-da7d-20296f2e32c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1 passed!\n",
            "Test Case 2 passed!\n",
            "Test Case 3 passed!\n",
            "Test Case 4 passed!\n",
            "Test Case 5 passed!\n",
            "Test Case 6 passed!\n",
            "Test Case 7 passed!\n",
            "Test Case 8 passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm `backward_propagation(dA, cache, activation)` thực hiện lan truyền ngược qua một lớp của mạng neural, giúp tính toán các gradient của hàm mất mát đối với các tham số của lớp đó (trọng số và bias), cũng như gradient đối với đầu vào của lớp. Đây là bước cần thiết để cập nhật các tham số trong quá trình huấn luyện.\n",
        "\n",
        "Trong hàm này:\n",
        "- `dA` là gradient của hàm mất mát đối với đầu ra của lớp hiện tại.\n",
        "- `cache` chứa các giá trị trung gian từ lan truyền xuôi, bao gồm đầu vào, trọng số, và đầu ra trước khi áp dụng hàm kích hoạt.\n",
        "- `activation` là loại hàm kích hoạt, có thể là `relu` hoặc `linear`, với các cách tính gradient khác nhau:\n",
        "  - Nếu hàm kích hoạt là `relu`, gradient được nhân với đạo hàm của ReLU.\n",
        "  - Nếu là `linear`, thì gradient đầu ra (`dZ`) giữ nguyên giá trị của `dA`."
      ],
      "metadata": {
        "id": "ZcWijRXu5r0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lan truyền ngược (một lớp)\n",
        "def backward_propagation(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Thực hiện lan truyền ngược qua một lớp của mạng neural.\n",
        "\n",
        "    Hàm này tính toán gradient của hàm mất mát đối với các tham số của lớp (trọng số và bias)\n",
        "    và gradient đối với đầu vào của lớp đó. Đây là bước cần thiết để cập nhật các tham số trong quá trình huấn luyện.\n",
        "\n",
        "    Tham số:\n",
        "    dA -- Gradient của hàm mất mát đối với đầu ra của lớp hiện tại, có kích thước (số lượng đơn vị lớp hiện tại, số lượng ví dụ).\n",
        "    cache -- Dictionary chứa các giá trị cần thiết cho lan truyền ngược, bao gồm:\n",
        "        - \"Z\" -- Giá trị trước kích hoạt của lớp hiện tại.\n",
        "        - \"A_prev\" -- Đầu vào của lớp hiện tại.\n",
        "        - \"W\" -- Trọng số của lớp hiện tại.\n",
        "        - \"A\" -- Đầu ra của lớp hiện tại sau khi áp dụng hàm kích hoạt.\n",
        "    activation -- Loại hàm kích hoạt, có thể là \"relu\" hoặc \"linear\".\n",
        "\n",
        "    Trả về:\n",
        "    dA_prev -- Gradient của hàm mất mát đối với đầu vào của lớp hiện tại, có kích thước (số lượng đặc trưng, số lượng ví dụ).\n",
        "    dW -- Gradient của hàm mất mát đối với ma trận trọng số của lớp hiện tại.\n",
        "    db -- Gradient của hàm mất mát đối với vector bias của lớp hiện tại.\n",
        "    \"\"\"\n",
        "    Z = cache[\"Z\"]\n",
        "\n",
        "    \"\"\"\n",
        "    Gợi ý:\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        ... # Nhân dA với đạo hàm của hàm ReLU\n",
        "    elif activation == \"linear\":\n",
        "        ...  # Đối với kích hoạt tuyến tính, dZ = dA\n",
        "    else:\n",
        "        raise ValueError(\"Hàm kích hoạt không được hỗ trợ\")\n",
        "\n",
        "    A_prev = cache[\"A_prev\"]\n",
        "    m = ... # Số lượng data của A_prev\n",
        "\n",
        "    dW = ... # Dựa trên công thức ở trên\n",
        "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(cache[\"W\"].T, dZ)\n",
        "    \"\"\"\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    if activation == \"relu\":\n",
        "        dZ = dA * relu_derivative(Z)\n",
        "    elif activation == \"linear\":\n",
        "        dZ = dA  # Đối với kích hoạt tuyến tính, dZ = dA\n",
        "    else:\n",
        "        raise ValueError(\"Hàm kích hoạt không được hỗ trợ\")\n",
        "\n",
        "    A_prev = cache[\"A_prev\"]\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(cache[\"W\"].T, dZ)\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "I5h6TqSrBDVf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests 10 points.\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "# Test Case 1: ReLU activation\n",
        "def test_case_1():\n",
        "    np.random.seed(1)\n",
        "    dA = np.random.randn(4, 2)\n",
        "    A_prev = np.random.randn(3, 2)\n",
        "    W = np.random.randn(4, 3)\n",
        "    b = np.random.randn(4, 1)\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    cache = {\"Z\": Z, \"A_prev\": A_prev, \"W\": W, \"A\": relu(Z)}\n",
        "\n",
        "    dA_prev, dW, db = backward_propagation(dA, cache, \"relu\")\n",
        "\n",
        "    assert dA_prev.shape == (3, 2), f\"dA_prev shape is incorrect. Expected (3, 2), but got {dA_prev.shape}\"\n",
        "    assert dW.shape == (4, 3), f\"dW shape is incorrect. Expected (4, 3), but got {dW.shape}\"\n",
        "    assert db.shape == (4, 1), f\"db shape is incorrect. Expected (4, 1), but got {db.shape}\"\n",
        "\n",
        "    # Manual calculation for comparison\n",
        "    dZ = dA * relu_derivative(Z)\n",
        "    expected_dW = (1 / 2) * np.dot(dZ, A_prev.T)\n",
        "    expected_db = (1 / 2) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    expected_dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    assert np.allclose(dW, expected_dW), \"dW calculation is incorrect\"\n",
        "    assert np.allclose(db, expected_db), \"db calculation is incorrect\"\n",
        "    assert np.allclose(dA_prev, expected_dA_prev), \"dA_prev calculation is incorrect\"\n",
        "\n",
        "    print(\"Test Case 1 passed!\")\n",
        "\n",
        "test_case_1()\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "\n",
        "# Test Case 2: Linear activation\n",
        "def test_case_2():\n",
        "    np.random.seed(2)\n",
        "    dA = np.random.randn(2, 3)\n",
        "    A_prev = np.random.randn(4, 3)\n",
        "    W = np.random.randn(2, 4)\n",
        "    b = np.random.randn(2, 1)\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    cache = {\"Z\": Z, \"A_prev\": A_prev, \"W\": W, \"A\": Z}\n",
        "\n",
        "    dA_prev, dW, db = backward_propagation(dA, cache, \"linear\")\n",
        "\n",
        "    assert dA_prev.shape == (4, 3), f\"dA_prev shape is incorrect. Expected (4, 3), but got {dA_prev.shape}\"\n",
        "    assert dW.shape == (2, 4), f\"dW shape is incorrect. Expected (2, 4), but got {dW.shape}\"\n",
        "    assert db.shape == (2, 1), f\"db shape is incorrect. Expected (2, 1), but got {db.shape}\"\n",
        "\n",
        "    # Manual calculation for comparison\n",
        "    expected_dW = (1 / 3) * np.dot(dA, A_prev.T)\n",
        "    expected_db = (1 / 3) * np.sum(dA, axis=1, keepdims=True)\n",
        "    expected_dA_prev = np.dot(W.T, dA)\n",
        "\n",
        "    assert np.allclose(dW, expected_dW), \"dW calculation is incorrect\"\n",
        "    assert np.allclose(db, expected_db), \"db calculation is incorrect\"\n",
        "    assert np.allclose(dA_prev, expected_dA_prev), \"dA_prev calculation is incorrect\"\n",
        "\n",
        "    print(\"Test Case 2 passed!\")\n",
        "\n",
        "# Test Case 3: Invalid activation function\n",
        "def test_case_3():\n",
        "    np.random.seed(3)\n",
        "    dA = np.random.randn(2, 2)\n",
        "    cache = {\"Z\": np.random.randn(2, 2), \"A_prev\": np.random.randn(3, 2), \"W\": np.random.randn(2, 3)}\n",
        "\n",
        "    try:\n",
        "        backward_propagation(dA, cache, \"invalid\")\n",
        "        assert False, \"Expected ValueError was not raised\"\n",
        "    except ValueError as e:\n",
        "        assert str(e) == \"Hàm kích hoạt không được hỗ trợ\", f\"Unexpected error message: {str(e)}\"\n",
        "\n",
        "    print(\"Test Case 3 passed!\")\n",
        "\n",
        "# Test Case 4: ReLU activation with zero input\n",
        "def test_case_4():\n",
        "    dA = np.array([[1, 2], [3, 4]])\n",
        "    A_prev = np.zeros((3, 2))\n",
        "    W = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "    b = np.array([[0.1], [0.2]])\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    cache = {\"Z\": Z, \"A_prev\": A_prev, \"W\": W, \"A\": relu(Z)}\n",
        "\n",
        "    dA_prev, dW, db = backward_propagation(dA, cache, \"relu\")\n",
        "\n",
        "    assert np.allclose(dW, 0), \"dW should be zero when A_prev is zero\"\n",
        "    assert np.allclose(db, np.array([[1.5], [3.5]])), \"db calculation is incorrect for zero input\"\n",
        "    assert np.allclose(dA_prev, np.array([[13, 18], [17, 24], [21, 30]])), \"dA_prev should be zero when all inputs are zero\"\n",
        "\n",
        "    print(\"Test Case 4 passed!\")\n",
        "\n",
        "# Test Case 5: Linear activation with large values\n",
        "def test_case_5():\n",
        "    np.random.seed(5)\n",
        "    dA = np.random.randn(2, 3) * 1000\n",
        "    A_prev = np.random.randn(4, 3) * 1000\n",
        "    W = np.random.randn(2, 4)\n",
        "    b = np.random.randn(2, 1)\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    cache = {\"Z\": Z, \"A_prev\": A_prev, \"W\": W, \"A\": Z}\n",
        "\n",
        "    dA_prev, dW, db = backward_propagation(dA, cache, \"linear\")\n",
        "\n",
        "    assert not np.any(np.isnan(dA_prev)), \"dA_prev contains NaN values\"\n",
        "    assert not np.any(np.isnan(dW)), \"dW contains NaN values\"\n",
        "    assert not np.any(np.isnan(db)), \"db contains NaN values\"\n",
        "\n",
        "    print(\"Test Case 5 passed!\")\n",
        "\n",
        "# Test Case 6: ReLU activation with negative Z values\n",
        "def test_case_6():\n",
        "    dA = np.array([[1, 2], [3, 4]])\n",
        "    A_prev = np.array([[-1, 2], [3, -4], [5, -6]])\n",
        "    W = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "    b = np.array([[-0.1], [-0.2]])\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    cache = {\"Z\": Z, \"A_prev\": A_prev, \"W\": W, \"A\": relu(Z)}\n",
        "\n",
        "    dA_prev, dW, db = backward_propagation(dA, cache, \"relu\")\n",
        "\n",
        "    expected_dZ = dA * (Z > 0)\n",
        "    assert np.allclose(np.dot(W.T, expected_dZ), dA_prev), \"dA_prev calculation is incorrect for ReLU with negative Z values\"\n",
        "\n",
        "    print(\"Test Case 6 passed!\")\n",
        "\n",
        "# Test Case 7: Linear activation with small learning rate simulation\n",
        "def test_case_7():\n",
        "    np.random.seed(7)\n",
        "    dA = np.random.randn(2, 3) * 0.01\n",
        "    A_prev = np.random.randn(4, 3)\n",
        "    W = np.random.randn(2, 4)\n",
        "    b = np.random.randn(2, 1)\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    cache = {\"Z\": Z, \"A_prev\": A_prev, \"W\": W, \"A\": Z}\n",
        "\n",
        "    dA_prev, dW, db = backward_propagation(dA, cache, \"linear\")\n",
        "\n",
        "    assert np.all(np.abs(dW) < 0.1), \"dW values are too large for small learning rate scenario\"\n",
        "    assert np.all(np.abs(db) < 0.1), \"db values are too large for small learning rate scenario\"\n",
        "\n",
        "    print(\"Test Case 7 passed!\")\n",
        "\n",
        "# Test Case 8: ReLU activation with batch size 1\n",
        "def test_case_8():\n",
        "    dA = np.array([[1], [2]])\n",
        "    A_prev = np.array([[3], [4], [5]])\n",
        "    W = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
        "    b = np.array([[0.01], [0.02]])\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    cache = {\"Z\": Z, \"A_prev\": A_prev, \"W\": W, \"A\": relu(Z)}\n",
        "\n",
        "    dA_prev, dW, db = backward_propagation(dA, cache, \"relu\")\n",
        "\n",
        "    assert dA_prev.shape == (3, 1), f\"dA_prev shape is incorrect. Expected (3, 1), but got {dA_prev.shape}\"\n",
        "    assert dW.shape == (2, 3), f\"dW shape is incorrect. Expected (2, 3), but got {dW.shape}\"\n",
        "    assert db.shape == (2, 1), f\"db shape is incorrect. Expected (2, 1), but got {db.shape}\"\n",
        "\n",
        "    print(\"Test Case 8 passed!\")\n",
        "\n",
        "test_case_2()\n",
        "test_case_3()\n",
        "test_case_4()\n",
        "test_case_5()\n",
        "test_case_6()\n",
        "test_case_7()\n",
        "test_case_8()\n",
        "\n",
        "### END HIDDEN TESTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNwae_dB4V_B",
        "outputId": "b202b655-6cee-4ba5-be86-98ca99519258"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1 passed!\n",
            "Test Case 2 passed!\n",
            "Test Case 3 passed!\n",
            "Test Case 4 passed!\n",
            "Test Case 5 passed!\n",
            "Test Case 6 passed!\n",
            "Test Case 7 passed!\n",
            "Test Case 8 passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BÀI TẬP: Cập nhật tham số"
      ],
      "metadata": {
        "id": "hNxKp0oZ6sMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Gradient Descent là một phương pháp tối ưu hóa được sử dụng để giảm thiểu hàm mất mát của mô hình. Bằng cách cập nhật các tham số theo hướng của gradient âm của hàm mất mát, gradient descent giúp tìm kiếm các tham số tối ưu nhằm giảm thiểu giá trị của hàm mất mát.\n",
        "\n",
        "### Các Bước Của Gradient Descent\n",
        "1. **Khởi tạo tham số**: Bắt đầu với các giá trị ban đầu cho các tham số (thường là các giá trị ngẫu nhiên hoặc số không).\n",
        "2. **Tính toán gradient**: Tính gradient của hàm mất mát đối với các tham số hiện tại.\n",
        "3. **Cập nhật tham số**: Điều chỉnh các tham số bằng cách trừ đi một phần của gradient, tỷ lệ với tốc độ học (learning rate).\n",
        "4. **Lặp lại**: Tiếp tục lặp lại các bước 2 và 3 cho đến khi đạt được hội tụ (giá trị hàm mất mát không thay đổi đáng kể) hoặc số lượng vòng lặp tối đa.\n",
        "\n",
        "### Công Thức Gradient Descent\n",
        "Giả sử hàm mất mát là $J(\\theta)$, gradient descent sẽ cập nhật tham số \\theta như sau:\n",
        "\\begin{equation*}\n",
        "\\theta := \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
        "\\end{equation*}\n",
        "trong đó:\n",
        "- $\\theta$ là vector tham số\n",
        "- $\\alpha$ là tốc độ học\n",
        "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ là gradient của hàm mất mát đối với tham số $\\theta$"
      ],
      "metadata": {
        "id": "SVzqr3mEtfTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Cập nhật tham số sử dụng gradient descent với gradient clipping.\n",
        "\n",
        "    Tham số:\n",
        "    parameters -- dictionary chứa các tham số\n",
        "    grads -- dictionary chứa các gradient\n",
        "    learning_rate -- tốc độ học của quy tắc cập nhật gradient descent\n",
        "\n",
        "    Trả về:\n",
        "    parameters -- dictionary chứa các tham số đã được cập nhật\n",
        "    \"\"\"\n",
        "    W = parameters[\"W\"].astype(float)\n",
        "    b = parameters[\"b\"].astype(float)\n",
        "    dW = grads[\"dW\"]\n",
        "    db = grads[\"db\"]\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    W -= learning_rate * dW\n",
        "    b -= learning_rate * db\n",
        "    ### END SOLUTION\n",
        "\n",
        "    parameters = {\"W\": W, \"b\": b}\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "wA_P2chO4XnF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests 10 points.\n",
        "\n",
        "# Test Case 1: Normal update without clipping\n",
        "def test_case_1():\n",
        "    np.random.seed(1)\n",
        "    parameters = {\n",
        "        \"W\": np.random.randn(3, 4),\n",
        "        \"b\": np.random.randn(3, 1)\n",
        "    }\n",
        "    grads = {\n",
        "        \"dW\": np.random.randn(3, 4) * 0.1,\n",
        "        \"db\": np.random.randn(3, 1) * 0.1\n",
        "    }\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    original_W = parameters[\"W\"].copy()\n",
        "    original_b = parameters[\"b\"].copy()\n",
        "\n",
        "    updated_parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    assert np.allclose(updated_parameters[\"W\"], original_W - learning_rate * grads[\"dW\"]), \"W update is incorrect\"\n",
        "    assert np.allclose(updated_parameters[\"b\"], original_b - learning_rate * grads[\"db\"]), \"b update is incorrect\"\n",
        "    print(\"Test Case 1 passed!\")\n",
        "\n",
        "test_case_1()\n",
        "\n",
        "### START HIDDEN TESTS\n",
        "\n",
        "# Test Case 2: Update with zero gradients\n",
        "def test_case_2():\n",
        "    parameters = {\n",
        "        \"W\": np.array([[1, 2], [3, 4]]),\n",
        "        \"b\": np.array([[5], [6]])\n",
        "    }\n",
        "    grads = {\n",
        "        \"dW\": np.zeros((2, 2)),\n",
        "        \"db\": np.zeros((2, 1))\n",
        "    }\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    updated_parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    assert np.array_equal(updated_parameters[\"W\"], parameters[\"W\"]), \"W should not change with zero gradients\"\n",
        "    assert np.array_equal(updated_parameters[\"b\"], parameters[\"b\"]), \"b should not change with zero gradients\"\n",
        "    print(\"Test Case 2 passed!\")\n",
        "\n",
        "# Test Case 5: Update with very small learning rate\n",
        "def test_case_3():\n",
        "    np.random.seed(5)\n",
        "    parameters = {\n",
        "        \"W\": np.random.randn(3, 4),\n",
        "        \"b\": np.random.randn(3, 1)\n",
        "    }\n",
        "    grads = {\n",
        "        \"dW\": np.random.randn(3, 4),\n",
        "        \"db\": np.random.randn(3, 1)\n",
        "    }\n",
        "    learning_rate = 1e-10\n",
        "\n",
        "    original_W = parameters[\"W\"].copy()\n",
        "    original_b = parameters[\"b\"].copy()\n",
        "\n",
        "    updated_parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    assert np.allclose(updated_parameters[\"W\"], original_W, atol=1e-8), \"W update with small learning rate is incorrect\"\n",
        "    assert np.allclose(updated_parameters[\"b\"], original_b, atol=1e-8), \"b update with small learning rate is incorrect\"\n",
        "    print(\"Test Case 3 passed!\")\n",
        "\n",
        "# Run all test cases\n",
        "test_case_2()\n",
        "test_case_3()\n",
        "\n",
        "### END HIDDEN TESTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSOovR21twSE",
        "outputId": "b19eddbf-a478-4aea-9d97-f17f5d89cd11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1 passed!\n",
            "Test Case 2 passed!\n",
            "Test Case 3 passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chạy thử cập nhật tham số"
      ],
      "metadata": {
        "id": "u1ZH2X3X6wAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phần code dưới đây thực hiện một bước lan truyền xuôi và lan truyền ngược qua một lớp của mạng neural đơn giản, sau đó cập nhật các tham số trọng số và bias bằng cách sử dụng gradient descent. Mô hình này giả định đầu ra liên tục, với hàm mất mát là lỗi trung bình bình phương (MSE). Quá trình bao gồm các bước chính:\n",
        "\n",
        "1. Khởi tạo dữ liệu ví dụ và tham số ban đầu.\n",
        "2. Lan truyền xuôi để tính đầu ra của lớp và chi phí.\n",
        "3. Lan truyền ngược để tính gradient của các tham số.\n",
        "4. Cập nhật tham số bằng cách sử dụng gradient descent."
      ],
      "metadata": {
        "id": "IlAPhywCmCQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Thiết lập dữ liệu ví dụ\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(3, 5)  # 3 đặc trưng, 5 ví dụ\n",
        "Y = np.random.randn(1, 5)  # Đầu ra liên tục cho MSE\n",
        "W = np.random.randn(1, 3)  # 1 neuron, 3 đặc trưng đầu vào\n",
        "b = np.zeros((1, 1))\n",
        "\n",
        "# Lan truyền xuôi\n",
        "Z = np.dot(W, X) + b\n",
        "A = relu(Z)  # Giả sử hàm relu đã được định nghĩa\n",
        "\n",
        "# Tính toán chi phí\n",
        "cost = compute_cost(A, Y)\n",
        "print(\"Chi phí ban đầu:\", cost)\n",
        "\n",
        "# Chuẩn bị cho lan truyền ngược\n",
        "dA = A - Y  # Gradient của MSE đối với A\n",
        "\n",
        "# Tạo cache\n",
        "cache = {\n",
        "    \"Z\": Z,\n",
        "    \"A_prev\": X,\n",
        "    \"W\": W,\n",
        "    \"A\": A\n",
        "}\n",
        "\n",
        "# Thực hiện lan truyền ngược\n",
        "dA_prev, dW, db = backward_propagation(dA, cache, \"relu\")\n",
        "\n",
        "# Chuẩn bị cập nhật tham số\n",
        "parameters = {\"W\": W, \"b\": b}\n",
        "grads = {\"dW\": dW, \"db\": db}\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Cập nhật tham số\n",
        "updated_parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "# In kết quả\n",
        "print(\"\\nGradient:\")\n",
        "print(\"dW:\", grads[\"dW\"])\n",
        "print(\"db:\", grads[\"db\"])\n",
        "print(\"\\nTham số đã cập nhật:\")\n",
        "print(\"W:\", updated_parameters[\"W\"])\n",
        "print(\"b:\", updated_parameters[\"b\"])"
      ],
      "metadata": {
        "id": "iW7zfBMd6xUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71d3e4e-3907-4ff4-93f4-2fb76da3bdc7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chi phí ban đầu: 1.4685984771390461\n",
            "\n",
            "Gradient:\n",
            "dW: [[ 1.14172833 -0.2807227  -1.29120827]]\n",
            "db: [[0.98140377]]\n",
            "\n",
            "Tham số đã cập nhật:\n",
            "W: [[ 1.45423149 -0.22296907  0.08044029]]\n",
            "b: [[-0.00981404]]\n"
          ]
        }
      ]
    }
  ]
}